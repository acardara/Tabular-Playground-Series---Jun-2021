{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular Playground Series - Jun 2021 [acardara]",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderCardarasUCSC/Tabular-Playground-Series---Jun-2021/blob/main/Tabular_Playground_Series_Jun_2021_%5Bacardara%5D(6-19).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti90w18y-uM7"
      },
      "source": [
        "### Introuction/Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f77-1WvV4cA8"
      },
      "source": [
        "Code entry for the **Tabular Playground Series - Jun 2021** kaggle competition.\n",
        "\n",
        "https://www.kaggle.com/c/tabular-playground-series-jun-2021/overview\n",
        "\n",
        "\n",
        "\n",
        "Inspiration for the code from the keras functional_api guide.\n",
        "\n",
        "https://keras.io/guides/functional_api/.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syOJbPyglvrw",
        "outputId": "745d88ea-9123-4653-d769-ed18bcb427c9"
      },
      "source": [
        "# mount google drive to download dataset\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFk07fPWmL_D",
        "outputId": "658ee579-82b0-4542-eded-098716da52de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a folder to store dataset files\n",
        "!mkdir /content/dataset\n",
        "\n",
        "# copy dataset from google drive\n",
        "!cp /content/gdrive/MyDrive/kaggle/train.csv /content/dataset\n",
        "!cp /content/gdrive/MyDrive/kaggle/test.csv /content/dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/dataset’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr8K3C4lTH_U"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def shuffle_data(data):\n",
        "  i = np.random.permutation(len(data))\n",
        "  new_data = data[i]\n",
        "  return new_data\n",
        "\n",
        "def shuffle_data_xyz(x, y, z):\n",
        "  i = np.random.permutation(len(x))\n",
        "  new_x = x[i]\n",
        "  new_y = y[i]\n",
        "  new_z = z[i]\n",
        "  return new_x, new_y, new_z\n",
        "\n",
        "def split_train_val(dataset, split=0.8, shuffle=True):\n",
        "  # shuffle rows in dataset\n",
        "  if shuffle:\n",
        "    dataset = shuffle_data(dataset)\n",
        "\n",
        "  train_set_length = int(len(dataset) * split)\n",
        "  print(type(train_set_length))\n",
        "\n",
        "  # split dataset into real sets of \n",
        "  train_set = dataset[0:train_set_length]\n",
        "  val_set = dataset[train_set_length:]\n",
        "  return train_set, val_set\n",
        "\n",
        "def labels_to_int(labels):\n",
        "# applying to_categorical is currently causing an error, \n",
        "# TODO: implement it correctly\n",
        "  new_labels = []\n",
        "  for label in labels:\n",
        "    # we want the 6th character of each class label string ie 'Class_7' should \n",
        "    # convert to '7'\n",
        "\n",
        "    # to_categorical expects values to start from 0, dataset is in range [1,9]\n",
        "    # we must subtract 1 from class labels for training to get the range [0,8]\n",
        "    new_labels.append(int(label[-1])-1)\n",
        "  new_labels = np.asarray(new_labels)\n",
        "  # return to_categorical(new_labels, NUMBER_OF_CLASSES)\n",
        "  return new_labels\n",
        "\n",
        "def split_data(dataset, is_test=False):\n",
        "  if not is_test:\n",
        "    id = dataset[:,0]\n",
        "    features = dataset[:,1:-1]\n",
        "    label = dataset[:,-1]\n",
        "    label = labels_to_int(label)\n",
        "  else:\n",
        "    id = dataset[:,0]\n",
        "    features = dataset[:,1:]\n",
        "    label = None\n",
        "  return id, features, label"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZymkdKknKX0",
        "outputId": "9fd81518-8463-4f48-ec46-edeb03d5204a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw_dataset_train = pd.read_csv(\"/content/dataset/train.csv\", header=0)\n",
        "np_dataset_train = raw_dataset_train.to_numpy()\n",
        "train, val = split_train_val(np_dataset_train)\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "\n",
        "# dataset_train = raw_dataset.to_numpy()[0:140_000]\n",
        "# dataset_validation = raw_dataset.to_numpy()[140_000:]\n",
        "\n",
        "# print(dataset_train.shape)\n",
        "# print(dataset_validation.shape)\n",
        "\n",
        "# raw_testset = pd.read_csv(\"/content/dataset/test.csv\", header=0)\n",
        "# raw_testset = raw_testset.to_numpy()\n",
        "# print(raw_testset.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'int'>\n",
            "(160000, 77)\n",
            "(40000, 77)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-L8LPG-1O1"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVXp8SKcojCu"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "NUMBER_OF_CLASSES = 9\n",
        "SUMMARIZE_AFTER = 1\n",
        "\n",
        "def generate_training_samples(dataset, n_samples):\n",
        "  train_ids, train_features, train_labels = split_data(dataset)\n",
        "  rows = np.random.permutation(len(train_ids))[0:n_samples]\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(0,9):\n",
        "    x.append([])\n",
        "    y.append([])\n",
        "\n",
        "  for row in rows:\n",
        "    #TODO maybe convert to int\n",
        "    label = train_labels[row]\n",
        "    features = train_features[row]\n",
        "\n",
        "    x[label].append(features)\n",
        "    y[label].append(label)\n",
        "\n",
        "  # x = np.asarray(x)\n",
        "  # y = np.asarray(y)\n",
        "\n",
        "  return x, y\n",
        "  # print(rows)\n",
        "\n",
        "  # # rows = [np.random.randint(0,dataset.shape[0]) for _ in range(n_samples)]\n",
        "  # samples = dataset[rows]\n",
        "\n",
        "  # train_x = samples[:,1:-1].astype(\"float32\")\n",
        "  # train_y = samples[:,-1]\n",
        "  # train_y = string_to_categorical(train_y).astype(\"float32\")\n",
        "  # return train_x, train_y\n",
        "\n",
        "# print accuracy of holdout dataset\n",
        "def summarize_performance(dataset_validation, model, epoch):\n",
        "  train_x, train_y = generate_training_samples(dataset_validation, dataset_validation.shape[0])\n",
        "  for i in range(0,9):\n",
        "    _loss, _accuracy = model.evaluate(train_x[i], train_y[i])\n",
        "    validation_loss[i].append(_loss)\n",
        "\n",
        "def train_model(dataset_train, dataset_validation, model, n_epochs, batch_size):\n",
        "\n",
        "  # train_ids, train_features, train_labels = split_data(dataset_train)\n",
        "  # val_ids, val_features, val_labels = split_data(dataset_validation)\n",
        "  iterations_per_epoch = dataset_train.shape[0]\n",
        "  batches_per_epoch = iterations_per_epoch//batch_size\n",
        "  for epoch in range(n_epochs):\n",
        "    temp_train_loss = []\n",
        "    for i in range(0,9):\n",
        "      temp_train_loss.append([])\n",
        "\n",
        "    for batch in range(batches_per_epoch):\n",
        "        train_x, train_y = generate_training_samples(dataset_train, batch_size)\n",
        "        for i in range(0,9):\n",
        "          if len(train_x[i]) == 0:\n",
        "            continue\n",
        "          _loss, _accuracy = model.train_on_batch(np.asarray(train_x[i]).astype(\"float32\"), np.asarray(train_y[i]).astype(\"float32\"))\n",
        "          temp_train_loss[i].append(_loss)\n",
        "\n",
        "          if batch % 200 == 0:\n",
        "            print(\">%d %d/%d, %.3f, %.3f\"%(epoch, batch, batches_per_epoch, _loss, _accuracy))\n",
        "\n",
        "        # print metrics every once in a while\n",
        "    # if (epoch+1) % SUMMARIZE_AFTER == 0:\n",
        "    for i in range(0,9):\n",
        "      train_loss[i].append(sum(temp_train_loss[i])/len(temp_train_loss[i]))\n",
        "      summarize_performance(dataset_validation, model, epoch)\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLjWd7qO_A5G"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeWWVGTB_AaO"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_baseline_model():\n",
        "  input_layer = Input(shape=(75,), name=\"First_Layer\")\n",
        "\n",
        "  output_layer = Dense(128, activation=\"relu\")(input_layer)\n",
        "  output_layer = Dropout(0.4)(output_layer)\n",
        "  \n",
        "  output_layer = Dense(64, activation=\"relu\")(output_layer)\n",
        "  output_layer = Dense(NUMBER_OF_CLASSES, activation=\"softmax\")(output_layer)\n",
        "  \n",
        "  model = Model(inputs=input_layer, outputs=output_layer, name=\"Simple_Classification_Model\")\n",
        "\n",
        "  loss = SparseCategoricalCrossentropy()\n",
        "  opt = Adam(learning_rate=0.0005, beta_1=0.5)\n",
        "  model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfoehBkU_FaJ"
      },
      "source": [
        "### Train Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oPG2bg_Dw4Qw",
        "outputId": "6f45f3cc-3e48-4c0a-8ff8-6725b76570b9"
      },
      "source": [
        "train_loss, validation_loss = [], []\n",
        "models = []\n",
        "for i in range(0,9):\n",
        "  train_loss.append([])\n",
        "  validation_loss.append([])\n",
        "\n",
        "n_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "model = define_baseline_model()\n",
        "\n",
        "model.summary()\n",
        "print(train.shape)\n",
        "train_model(train, val, model, n_epochs, batch_size)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Simple_Classification_Model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "First_Layer (InputLayer)     [(None, 75)]              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 128)               9728      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 9)                 585       \n",
            "=================================================================\n",
            "Total params: 18,569\n",
            "Trainable params: 18,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(160000, 77)\n",
            ">0 0/1600, 2.474, 0.167\n",
            ">0 0/1600, 10.056, 0.000\n",
            ">0 0/1600, 2.954, 0.444\n",
            ">0 0/1600, 5.692, 0.000\n",
            ">0 0/1600, 12.135, 0.000\n",
            ">0 0/1600, 4.351, 0.000\n",
            ">0 0/1600, 5.265, 0.000\n",
            ">0 0/1600, 4.806, 0.133\n",
            ">0 0/1600, 3.213, 0.000\n",
            ">0 200/1600, 2.143, 0.250\n",
            ">0 200/1600, 2.411, 0.333\n",
            ">0 200/1600, 2.329, 0.000\n",
            ">0 200/1600, 2.502, 0.000\n",
            ">0 200/1600, 2.912, 0.000\n",
            ">0 200/1600, 2.174, 0.042\n",
            ">0 200/1600, 1.735, 0.286\n",
            ">0 200/1600, 1.989, 0.231\n",
            ">0 200/1600, 2.362, 0.000\n",
            ">0 400/1600, 2.092, 0.167\n",
            ">0 400/1600, 2.132, 0.400\n",
            ">0 400/1600, 2.334, 0.000\n",
            ">0 400/1600, 2.154, 0.148\n",
            ">0 400/1600, 2.060, 0.333\n",
            ">0 400/1600, 2.128, 0.043\n",
            ">0 400/1600, 2.207, 0.154\n",
            ">0 600/1600, 2.116, 1.000\n",
            ">0 600/1600, 2.033, 0.556\n",
            ">0 600/1600, 2.282, 0.091\n",
            ">0 600/1600, 2.115, 0.000\n",
            ">0 600/1600, 2.212, 0.000\n",
            ">0 600/1600, 2.125, 0.238\n",
            ">0 600/1600, 2.222, 0.143\n",
            ">0 600/1600, 2.119, 0.143\n",
            ">0 600/1600, 2.280, 0.000\n",
            ">0 800/1600, 2.288, 0.000\n",
            ">0 800/1600, 2.006, 0.692\n",
            ">0 800/1600, 2.085, 0.000\n",
            ">0 800/1600, 2.416, 0.000\n",
            ">0 800/1600, 2.066, 0.345\n",
            ">0 800/1600, 2.208, 0.000\n",
            ">0 800/1600, 2.120, 0.190\n",
            ">0 800/1600, 2.137, 0.059\n",
            ">0 1000/1600, 2.127, 0.200\n",
            ">0 1000/1600, 1.985, 0.812\n",
            ">0 1000/1600, 2.015, 0.200\n",
            ">0 1000/1600, 2.529, 0.000\n",
            ">0 1000/1600, 2.284, 0.000\n",
            ">0 1000/1600, 2.088, 0.370\n",
            ">0 1000/1600, 2.183, 0.167\n",
            ">0 1000/1600, 2.183, 0.160\n",
            ">0 1000/1600, 2.172, 0.000\n",
            ">0 1200/1600, 2.065, 0.200\n",
            ">0 1200/1600, 1.757, 0.833\n",
            ">0 1200/1600, 2.013, 0.000\n",
            ">0 1200/1600, 2.089, 0.484\n",
            ">0 1200/1600, 2.312, 0.000\n",
            ">0 1200/1600, 2.092, 0.185\n",
            ">0 1200/1600, 2.174, 0.000\n",
            ">0 1400/1600, 2.143, 0.167\n",
            ">0 1400/1600, 1.753, 0.571\n",
            ">0 1400/1600, 2.123, 0.000\n",
            ">0 1400/1600, 2.199, 0.250\n",
            ">0 1400/1600, 2.064, 0.357\n",
            ">0 1400/1600, 1.984, 0.333\n",
            ">0 1400/1600, 2.130, 0.147\n",
            ">0 1400/1600, 2.319, 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3f7f541e4ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-5b6aa9623a15>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataset_train, dataset_validation, model, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_train_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_train_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0msummarize_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-5b6aa9623a15>\u001b[0m in \u001b[0;36msummarize_performance\u001b[0;34m(dataset_validation, model, epoch)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_training_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1464\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m     self._adapter = adapter_cls(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 994\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    995\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEE5sIbvpdPG"
      },
      "source": [
        "### Visualize losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy8wRl94cmVO"
      },
      "source": [
        "# sqrt mean(value)\n",
        "n = 10\n",
        "checkpoint = [trained_models, validation_losses]\n",
        "if current_validation_loss is not a minima for the last n epochs:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRnMb_tBMde0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x = [SUMMARIZE_AFTER*i for i in range(len(train_loss))]\n",
        "plt.plot(x, train_loss)\n",
        "plt.plot(x, validation_loss)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERPHrQmFnrdj"
      },
      "source": [
        "### Run Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ls4OhQnyYO"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def predict_class(model, features):\n",
        "  features = np.expand_dims(features, axis=0)\n",
        "  return model.predict(features)\n",
        "\n",
        "def predict_testset(model, testset):\n",
        "  labels = [\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"]\n",
        "  \n",
        "  results = []\n",
        "  ids = testset[:,0]\n",
        "  ids = ids.reshape((len(ids),1));\n",
        "  ids = pd.DataFrame(data=ids, columns=[labels[0]])\n",
        "\n",
        "  data = testset[:,1:]\n",
        "  results = model.predict(data)\n",
        "  results = pd.DataFrame(data=results, columns=labels[1:])\n",
        "\n",
        "  final = pd.concat([ids, results], axis=1)\n",
        "  # final = np.hstack((ids,results))\n",
        "\n",
        "  # return pd.DataFrame(data=final, columns=labels)\n",
        "  return final\n",
        "\n",
        "\n",
        "results = predict_testset(model, raw_testset)\n",
        "results.to_csv(\"submission.csv\", index=False)\n",
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZfu_PgDMrBW"
      },
      "source": [
        "### UMAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ct7HXMTMLxJ"
      },
      "source": [
        "!pip install umap-learn\n",
        "!pip install babyplots\n",
        "!pip install hdbscan "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG7FnLQB6WXZ"
      },
      "source": [
        "import umap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from babyplots import Babyplot\n",
        "import hdbscan\n",
        "\n",
        "\n",
        "\n",
        "dataset = raw_dataset.to_numpy()[:,1:-1]\n",
        "labels = raw_dataset.to_numpy()[:,-1]\n",
        "\n",
        "dataset = StandardScaler().fit_transform(dataset)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "class_names = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\", \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\"]\n",
        "le.fit(class_names)\n",
        "labels = le.transform(labels)\n",
        "\n",
        "print(labels)\n",
        "print(dataset)\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.30)\n",
        "reducer2d = umap.UMAP(n_neighbors=20,\n",
        "                        min_dist=0.0,\n",
        "                        n_components=2,\n",
        "                        random_state=42, metric='euclidean', target_weight=0)\n",
        "\n",
        "reducer3d = umap.UMAP(n_neighbors=20,\n",
        "                      min_dist=0.0,\n",
        "                      n_components=3,\n",
        "                      random_state=42, metric='euclidean', target_weight=0)\n",
        "\n",
        "# mapper2d = reducer2d.fit(X_train, y_train)\n",
        "mapper3d = reducer3d.fit(X_train, y_train)\n",
        "\n",
        "# embedding2d_train = mapper2d.transform(X_train)\n",
        "embedding3d_train = mapper3d.transform(X_train)\n",
        "\n",
        "\n",
        "\n",
        "# embedding2d_test = mapper2d.transform(X_test)\n",
        "embedding3d_test = mapper3d.transform(X_test)\n",
        "\n",
        "# embedding2d = embedding2d_train.tolist() + embedding2d_test.tolist()\n",
        "embedding3d = embedding3d_train.tolist() + embedding3d_test.tolist()\n",
        "# y_test \n",
        "print(y_test.shape)\n",
        "# y_test = np.where(y_test >= 1, 'RI', 'DE')\n",
        "y_plot = y_train.tolist() + y_test.tolist()\n",
        "bp = Babyplot(background_color=\"#ffffddff\", )\n",
        "bp.add_plot(embedding3d, 'pointCloud', 'categories', y_plot, {\n",
        "    'colorScale': 'Viridis',\n",
        "    'showLegend': True,\n",
        "    'folded': True,\n",
        "    'size': 5,\n",
        "    'showAxes': [True, True, True],\n",
        "    'axisLabels': ['X', 'Y', 'Z'],\n",
        "    'showTickLines': [[True, True], [True, True], [True, True]]\n",
        "})\n",
        "bp.save_as_html(r'/content/test3.html')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU9_sM1Amhg3"
      },
      "source": [
        "import hdbscan\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dataset = raw_dataset.to_numpy()[:,1:-1]\n",
        "labels = raw_dataset.to_numpy()[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tFXKnlQmicB"
      },
      "source": [
        "hdbscan_labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500).fit_predict(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}