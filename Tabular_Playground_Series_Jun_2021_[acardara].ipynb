{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular Playground Series - Jun 2021 [acardara]",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderCardarasUCSC/Tabular-Playground-Series---Jun-2021/blob/main/Tabular_Playground_Series_Jun_2021_%5Bacardara%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti90w18y-uM7"
      },
      "source": [
        "### Introuction/Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f77-1WvV4cA8"
      },
      "source": [
        "Code entry for the **Tabular Playground Series - Jun 2021** kaggle competition.\n",
        "\n",
        "https://www.kaggle.com/c/tabular-playground-series-jun-2021/overview\n",
        "\n",
        "\n",
        "\n",
        "Inspiration for the code from the keras functional_api guide.\n",
        "\n",
        "https://keras.io/guides/functional_api/.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syOJbPyglvrw"
      },
      "source": [
        "# mount google drive to download dataset\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFk07fPWmL_D"
      },
      "source": [
        "# create a folder to store dataset files\n",
        "!mkdir /content/dataset\n",
        "\n",
        "# copy dataset from google drive\n",
        "!cp /content/gdrive/MyDrive/kaggle/train.csv /content/dataset\n",
        "!cp /content/gdrive/MyDrive/kaggle/test.csv /content/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZymkdKknKX0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "raw_dataset = pd.read_csv(\"/content/dataset/train.csv\", header=0)\n",
        "dataset_train = raw_dataset.to_numpy()[0:140_000]\n",
        "dataset_validation = raw_dataset.to_numpy()[140_000:]\n",
        "\n",
        "print(dataset_train.shape)\n",
        "print(dataset_validation.shape)\n",
        "\n",
        "raw_testset = pd.read_csv(\"/content/dataset/test.csv\", header=0)\n",
        "raw_testset = raw_testset.to_numpy()\n",
        "print(raw_testset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-L8LPG-1O1"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVXp8SKcojCu"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "NUMBER_OF_CLASSES = 9\n",
        "SUMMARIZE_AFTER = 1\n",
        "\n",
        "# applying to_categorical is currently causing an error, \n",
        "# TODO: implement it correctly\n",
        "def string_to_categorical(labels):\n",
        "  new_labels = []\n",
        "  for label in labels:\n",
        "    # we want the 6th character of each class label string ie 'Class_7' should \n",
        "    # convert to '7'\n",
        "\n",
        "    # to_categorical expects values to start from 0, dataset is in range [1,9]\n",
        "    # we must subtract 1 from class labels for training to get the range [0,8]\n",
        "    new_labels.append(int(label[0][6])-1)\n",
        "  new_labels = np.asarray(new_labels)\n",
        "  # return to_categorical(new_labels, NUMBER_OF_CLASSES)\n",
        "  return new_labels\n",
        "\n",
        "def generate_training_samples(dataset, n_samples):\n",
        "  rows = [np.random.randint(0,dataset.shape[0]) for _ in range(n_samples)]\n",
        "  samples = dataset[rows]\n",
        "\n",
        "  train_x = samples[:,1:-1].astype(\"float32\")\n",
        "  train_y = samples[:,-1:]\n",
        "  train_y = string_to_categorical(train_y).astype(\"float32\")\n",
        "  return train_x, train_y\n",
        "\n",
        "# print accuracy of holdout dataset\n",
        "def summarize_performance(dataset_validation, model, epoch):\n",
        "  train_x, train_y = generate_training_samples(dataset_validation, dataset_validation.shape[0])\n",
        "  _loss, _accuracy = model.evaluate(train_x, train_y)\n",
        "  validation_loss.append(_loss)\n",
        "\n",
        "def train(dataset_train, dataset_validation, model, n_epochs, batch_size):\n",
        "  iterations_per_epoch = dataset_train.shape[0]\n",
        "  batches_per_epoch = iterations_per_epoch//batch_size\n",
        "  for epoch in range(n_epochs):\n",
        "    temp_train_loss = []\n",
        "    for batch in range(batches_per_epoch):\n",
        "        train_x, train_y = generate_training_samples(dataset_train, batch_size)\n",
        "\n",
        "        _loss, _accuracy = model.train_on_batch(train_x, train_y)\n",
        "        temp_train_loss.append(_loss)\n",
        "\n",
        "        # print metrics every once in a while\n",
        "        if batch % 200 == 0:\n",
        "          print(\">%d %d/%d, %.3f, %.3f\"%(epoch, batch, batches_per_epoch, _loss, _accuracy))\n",
        "    if (epoch+1) % SUMMARIZE_AFTER == 0:\n",
        "      train_loss.append(sum(temp_train_loss)/len(temp_train_loss))\n",
        "      summarize_performance(dataset_validation, model, epoch)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLjWd7qO_A5G"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeWWVGTB_AaO"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_model():\n",
        "  input_layer = Input(shape=(75,), name=\"First_Layer\")\n",
        "\n",
        "  output_layer = Dense(128, activation=\"relu\")(input_layer)\n",
        "  output_layer = Dropout(0.4)(output_layer)\n",
        "  output_layer = Dense(64, activation=\"relu\")(output_layer)\n",
        "  output_layer = Dense(NUMBER_OF_CLASSES, activation=\"softmax\")(output_layer)\n",
        "  \n",
        "  model = Model(inputs=input_layer, outputs=output_layer, name=\"Simple_Classification_Model\")\n",
        "\n",
        "  loss = SparseCategoricalCrossentropy()\n",
        "  opt = Adam(learning_rate=0.0005, beta_1=0.5)\n",
        "  model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfoehBkU_FaJ"
      },
      "source": [
        "### Train Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPG2bg_Dw4Qw"
      },
      "source": [
        "train_loss, validation_loss = [], []\n",
        "\n",
        "n_epochs = 7\n",
        "batch_size = 100\n",
        "\n",
        "model = define_model()\n",
        "model.summary()\n",
        "train(dataset_train, dataset_validation, model, n_epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEE5sIbvpdPG"
      },
      "source": [
        "### Visualize losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRnMb_tBMde0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x = [SUMMARIZE_AFTER*i for i in range(len(train_loss))]\n",
        "plt.plot(x, train_loss)\n",
        "plt.plot(x, validation_loss)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERPHrQmFnrdj"
      },
      "source": [
        "### Run Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ls4OhQnyYO"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def predict_class(model, features):\n",
        "  features = np.expand_dims(features, axis=0)\n",
        "  return model.predict(features)\n",
        "\n",
        "def predict_testset(model, testset):\n",
        "  labels = [\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"]\n",
        "  \n",
        "  results = []\n",
        "  ids = testset[:,0]\n",
        "  ids = ids.reshape((len(ids),1));\n",
        "  ids = pd.DataFrame(data=ids, columns=[labels[0]])\n",
        "\n",
        "  data = testset[:,1:]\n",
        "  results = model.predict(data)\n",
        "  results = pd.DataFrame(data=results, columns=labels[1:])\n",
        "\n",
        "  final = pd.concat([ids, results], axis=1)\n",
        "  # final = np.hstack((ids,results))\n",
        "\n",
        "  # return pd.DataFrame(data=final, columns=labels)\n",
        "  return final\n",
        "\n",
        "\n",
        "results = predict_testset(model, raw_testset)\n",
        "results.to_csv(\"submission.csv\", index=False)\n",
        "# print(results)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZfu_PgDMrBW"
      },
      "source": [
        "### UMAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ct7HXMTMLxJ"
      },
      "source": [
        "!pip install umap-learn\n",
        "!pip install babyplots\n",
        "!pip install hdbscan "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG7FnLQB6WXZ"
      },
      "source": [
        "import umap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from babyplots import Babyplot\n",
        "import hdbscan\n",
        "\n",
        "\n",
        "\n",
        "dataset = raw_dataset.to_numpy()[:,1:-1]\n",
        "labels = raw_dataset.to_numpy()[:,-1]\n",
        "\n",
        "dataset = StandardScaler().fit_transform(dataset)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "class_names = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\", \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\"]\n",
        "le.fit(class_names)\n",
        "labels = le.transform(labels)\n",
        "\n",
        "print(labels)\n",
        "print(dataset)\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.30)\n",
        "reducer2d = umap.UMAP(n_neighbors=20,\n",
        "                        min_dist=0.0,\n",
        "                        n_components=2,\n",
        "                        random_state=42, metric='euclidean', target_weight=0)\n",
        "\n",
        "reducer3d = umap.UMAP(n_neighbors=20,\n",
        "                      min_dist=0.0,\n",
        "                      n_components=3,\n",
        "                      random_state=42, metric='euclidean', target_weight=0)\n",
        "\n",
        "# mapper2d = reducer2d.fit(X_train, y_train)\n",
        "mapper3d = reducer3d.fit(X_train, y_train)\n",
        "\n",
        "# embedding2d_train = mapper2d.transform(X_train)\n",
        "embedding3d_train = mapper3d.transform(X_train)\n",
        "\n",
        "\n",
        "\n",
        "# embedding2d_test = mapper2d.transform(X_test)\n",
        "embedding3d_test = mapper3d.transform(X_test)\n",
        "\n",
        "# embedding2d = embedding2d_train.tolist() + embedding2d_test.tolist()\n",
        "embedding3d = embedding3d_train.tolist() + embedding3d_test.tolist()\n",
        "# y_test \n",
        "print(y_test.shape)\n",
        "# y_test = np.where(y_test >= 1, 'RI', 'DE')\n",
        "y_plot = y_train.tolist() + y_test.tolist()\n",
        "bp = Babyplot(background_color=\"#ffffddff\", )\n",
        "bp.add_plot(embedding3d, 'pointCloud', 'categories', y_plot, {\n",
        "    'colorScale': 'Viridis',\n",
        "    'showLegend': True,\n",
        "    'folded': True,\n",
        "    'size': 5,\n",
        "    'showAxes': [True, True, True],\n",
        "    'axisLabels': ['X', 'Y', 'Z'],\n",
        "    'showTickLines': [[True, True], [True, True], [True, True]]\n",
        "})\n",
        "bp.save_as_html(r'/content/test3.html')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU9_sM1Amhg3"
      },
      "source": [
        "import hdbscan\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dataset = raw_dataset.to_numpy()[:,1:-1]\n",
        "labels = raw_dataset.to_numpy()[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.30)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tFXKnlQmicB"
      },
      "source": [
        "hdbscan_labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500).fit_predict(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}