{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular Playground Series - Jun 2021 [acardara]",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderCardarasUCSC/Tabular-Playground-Series---Jun-2021/blob/main/Tabular_Playground_Series_Jun_2021_%5Bacardara%5D%20(6-16).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti90w18y-uM7"
      },
      "source": [
        "### Introuction/Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f77-1WvV4cA8"
      },
      "source": [
        "Code entry for the **Tabular Playground Series - Jun 2021** kaggle competition.\n",
        "\n",
        "https://www.kaggle.com/c/tabular-playground-series-jun-2021/overview\n",
        "\n",
        "\n",
        "\n",
        "Inspiration for the code from the keras functional_api guide.\n",
        "\n",
        "https://keras.io/guides/functional_api/.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syOJbPyglvrw",
        "outputId": "3fe86465-33a5-4315-fe2c-4d77cb81fe49"
      },
      "source": [
        "# mount google drive to download dataset\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFk07fPWmL_D",
        "outputId": "aa06f72f-d1c7-4e0d-ad52-7c81f2c80139",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a folder to store dataset files\n",
        "!mkdir /content/dataset\n",
        "\n",
        "# copy dataset from google drive\n",
        "!cp /content/gdrive/MyDrive/kaggle/train.csv /content/dataset\n",
        "!cp /content/gdrive/MyDrive/kaggle/test.csv /content/dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/dataset’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZymkdKknKX0",
        "outputId": "0e0af542-a44b-47d0-8f85-3542ea9a1b29"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_dictionary(dataset):\n",
        "  dictionary = {}\n",
        "  labels = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\", \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\"]\n",
        "  for label in labels:\n",
        "    dictionary[label] = []\n",
        "\n",
        "  for i in range(dataset.shape[0]):\n",
        "    row = dataset[i]\n",
        "    dictionary[row[-1]].append(row[1:-1])\n",
        "\n",
        "  for label in labels:\n",
        "    dictionary[label] = np.asarray(dictionary[label])\n",
        "\n",
        "  return dictionary\n",
        "\n",
        "\n",
        "raw_dataset = pd.read_csv(\"/content/dataset/train.csv\", header=0)\n",
        "\n",
        "dataset_train = raw_dataset.to_numpy()[0:140_000]\n",
        "dataset_train = create_dictionary(dataset_train)\n",
        "\n",
        "dataset_validation = raw_dataset.to_numpy()[140_000:]\n",
        "dataset_validation = create_dictionary(dataset_validation)\n",
        "\n",
        "# print(dataset_train.shape)\n",
        "# print(dataset_validation.shape)\n",
        "\n",
        "raw_testset = pd.read_csv(\"/content/dataset/test.csv\", header=0)\n",
        "raw_testset = raw_testset.to_numpy()\n",
        "print(raw_testset.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 76)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-L8LPG-1O1"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVXp8SKcojCu"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "NUMBER_OF_CLASSES = 9\n",
        "SUMMARIZE_AFTER = 1\n",
        "\n",
        "# applying to_categorical is currently causing an error, \n",
        "# TODO: implement it correctly\n",
        "def string_to_categorical(labels):\n",
        "  new_labels = []\n",
        "  for label in labels:\n",
        "    # we want the 6th character of each class label string ie 'Class_7' should \n",
        "    # convert to '7'\n",
        "\n",
        "    # to_categorical expects values to start from 0, dataset is in range [1,9]\n",
        "    # we must subtract 1 from class labels for training to get the range [0,8]\n",
        "    new_labels.append(int(label[0][6])-1)\n",
        "  new_labels = np.asarray(new_labels)\n",
        "  # return to_categorical(new_labels, NUMBER_OF_CLASSES)\n",
        "  return new_labels\n",
        "\n",
        "def unison_shuffled_copies(a, b):\n",
        "  assert len(a) == len(b)\n",
        "  p = np.random.permutation(len(a))\n",
        "  return a[p], b[p]\n",
        "\n",
        "# TODO: use non-random class selection\n",
        "def generate_training_samples(dataset, n_samples):\n",
        "  samples_per_class = n_samples//NUMBER_OF_CLASSES\n",
        "\n",
        "  train_x = []\n",
        "  train_y = []\n",
        "  \n",
        "  for key in dataset:\n",
        "    # rows = [np.random.randint(0,len(dataset[key])) for _ in range(samples_per_class)]\n",
        "    rows = np.random.permutation(len(dataset[key]))[0:samples_per_class]\n",
        "\n",
        "    for data in dataset[key][rows]:\n",
        "      train_x.append(data)\n",
        "      train_y.append(int(key[-1])-1)\n",
        "\n",
        "  train_x = np.asarray(train_x).astype(\"float32\")\n",
        "  train_y = np.asarray(train_y).astype(\"float32\")\n",
        "\n",
        "  train_x, train_y = unison_shuffled_copies(train_x, train_y)\n",
        "  # print(train_x, train_y)\n",
        "\n",
        "  # train_y = samples[:,-1:]\n",
        "  # train_y = string_to_categorical(train_y).astype(\"float32\")\n",
        "  return train_x, train_y\n",
        "\n",
        "# print accuracy of holdout dataset\n",
        "def summarize_performance(dataset_validation, model, epoch):\n",
        "  train_x, train_y = generate_training_samples(dataset_validation, 60_000)\n",
        "  _loss, _accuracy = model.evaluate(train_x, train_y)\n",
        "  validation_loss.append(_loss)\n",
        "\n",
        "def train(dataset_train, dataset_validation, model, n_epochs, batch_size):\n",
        "  iterations_per_epoch = 140_000\n",
        "  batches_per_epoch = iterations_per_epoch//batch_size\n",
        "  for epoch in range(n_epochs):\n",
        "    temp_train_loss = []\n",
        "    for batch in range(batches_per_epoch):\n",
        "        train_x, train_y = generate_training_samples(dataset_train, batch_size)\n",
        "\n",
        "        _loss, _accuracy = model.train_on_batch(train_x, train_y)\n",
        "        temp_train_loss.append(_loss)\n",
        "\n",
        "        # print metrics every once in a while\n",
        "        if batch % 200 == 0:\n",
        "          print(\">%d %d/%d, %.3f, %.3f\"%(epoch, batch, batches_per_epoch, _loss, _accuracy))\n",
        "    if (epoch+1) % SUMMARIZE_AFTER == 0:\n",
        "      train_loss.append(sum(temp_train_loss)/len(temp_train_loss))\n",
        "      summarize_performance(dataset_validation, model, epoch)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLjWd7qO_A5G"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeWWVGTB_AaO"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_baseline_model():\n",
        "  input_layer = Input(shape=(75,), name=\"First_Layer\")\n",
        "\n",
        "  output_layer = Dense(128, activation=\"relu\")(input_layer)\n",
        "  output_layer = Dropout(0.4)(output_layer)\n",
        "  \n",
        "  output_layer = Dense(64, activation=\"relu\")(output_layer)\n",
        "\n",
        "  output_layer = Dense(NUMBER_OF_CLASSES, activation=\"softmax\")(output_layer)\n",
        "  \n",
        "  model = Model(inputs=input_layer, outputs=output_layer, name=\"Simple_Classification_Model\")\n",
        "\n",
        "  loss = SparseCategoricalCrossentropy()\n",
        "  opt = Adam(learning_rate=0.0005, beta_1=0.5)\n",
        "  model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "  return model\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfoehBkU_FaJ"
      },
      "source": [
        "### Train Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPG2bg_Dw4Qw",
        "outputId": "19cab61b-e8a4-46f7-edf0-90242e557334"
      },
      "source": [
        "train_loss, validation_loss = [], []\n",
        "\n",
        "n_epochs = 15\n",
        "batch_size = 10000\n",
        "\n",
        "model = define_baseline_model()\n",
        "# model = define_convolutional_model()\n",
        "\n",
        "model.summary()\n",
        "train(dataset_train, dataset_validation, model, n_epochs, batch_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Simple_Classification_Model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "First_Layer (InputLayer)     [(None, 75)]              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               9728      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 9)                 585       \n",
            "=================================================================\n",
            "Total params: 18,569\n",
            "Trainable params: 18,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            ">0 0/14, 6.381, 0.108\n",
            "1266/1266 [==============================] - 2s 1ms/step - loss: 3.1770 - accuracy: 0.1005\n",
            ">1 0/14, 3.353, 0.103\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.8390 - accuracy: 0.0983\n",
            ">2 0/14, 2.963, 0.102\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.6528 - accuracy: 0.0972\n",
            ">3 0/14, 2.751, 0.101\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.5267 - accuracy: 0.1007\n",
            ">4 0/14, 2.599, 0.104\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.4490 - accuracy: 0.1018\n",
            ">5 0/14, 2.509, 0.105\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.3868 - accuracy: 0.1036\n",
            ">6 0/14, 2.436, 0.106\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.3339 - accuracy: 0.1098\n",
            ">7 0/14, 2.375, 0.112\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.2989 - accuracy: 0.1142\n",
            ">8 0/14, 2.328, 0.116\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.2700 - accuracy: 0.1164\n",
            ">9 0/14, 2.297, 0.119\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.2462 - accuracy: 0.1210\n",
            ">10 0/14, 2.266, 0.121\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.2302 - accuracy: 0.1250\n",
            ">11 0/14, 2.247, 0.126\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.2186 - accuracy: 0.1325\n",
            ">12 0/14, 2.233, 0.131\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.2063 - accuracy: 0.1428\n",
            ">13 0/14, 2.220, 0.140\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.2007 - accuracy: 0.1497\n",
            ">14 0/14, 2.212, 0.146\n",
            "1266/1266 [==============================] - 1s 1ms/step - loss: 2.1961 - accuracy: 0.1579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEE5sIbvpdPG"
      },
      "source": [
        "### Visualize losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "jRnMb_tBMde0",
        "outputId": "44ce4e70-2181-4c3b-f34c-19f8d6d150fb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x = [SUMMARIZE_AFTER*i for i in range(len(train_loss))]\n",
        "plt.plot(x, train_loss)\n",
        "plt.plot(x, validation_loss)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERPHrQmFnrdj"
      },
      "source": [
        "### Run Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ls4OhQnyYO"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def predict_class(model, features):\n",
        "  features = np.expand_dims(features, axis=0)\n",
        "  return model.predict(features)\n",
        "\n",
        "def predict_testset(model, testset):\n",
        "  labels = [\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"]\n",
        "  \n",
        "  results = []\n",
        "  ids = testset[:,0]\n",
        "  ids = ids.reshape((len(ids),1));\n",
        "  ids = pd.DataFrame(data=ids, columns=[labels[0]])\n",
        "\n",
        "  data = testset[:,1:]\n",
        "  results = model.predict(data)\n",
        "  results = pd.DataFrame(data=results, columns=labels[1:])\n",
        "\n",
        "  final = pd.concat([ids, results], axis=1)\n",
        "  # final = np.hstack((ids,results))\n",
        "\n",
        "  # return pd.DataFrame(data=final, columns=labels)\n",
        "  return final\n",
        "\n",
        "\n",
        "results = predict_testset(model, raw_testset)\n",
        "results.to_csv(\"submission.csv\", index=False)\n",
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZfu_PgDMrBW"
      },
      "source": [
        "### UMAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ct7HXMTMLxJ"
      },
      "source": [
        "!pip install umap-learn\n",
        "!pip install babyplots\n",
        "!pip install hdbscan "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG7FnLQB6WXZ"
      },
      "source": [
        "import umap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from babyplots import Babyplot\n",
        "import hdbscan\n",
        "\n",
        "\n",
        "\n",
        "dataset = raw_dataset.to_numpy()[:,1:-1]\n",
        "labels = raw_dataset.to_numpy()[:,-1]\n",
        "\n",
        "dataset = StandardScaler().fit_transform(dataset)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "class_names = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\", \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\"]\n",
        "le.fit(class_names)\n",
        "labels = le.transform(labels)\n",
        "\n",
        "print(labels)\n",
        "print(dataset)\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.30)\n",
        "reducer2d = umap.UMAP(n_neighbors=20,\n",
        "                        min_dist=0.0,\n",
        "                        n_components=2,\n",
        "                        random_state=42, metric='euclidean', target_weight=0)\n",
        "\n",
        "reducer3d = umap.UMAP(n_neighbors=20,\n",
        "                      min_dist=0.0,\n",
        "                      n_components=3,\n",
        "                      random_state=42, metric='euclidean', target_weight=0)\n",
        "\n",
        "# mapper2d = reducer2d.fit(X_train, y_train)\n",
        "mapper3d = reducer3d.fit(X_train, y_train)\n",
        "\n",
        "# embedding2d_train = mapper2d.transform(X_train)\n",
        "embedding3d_train = mapper3d.transform(X_train)\n",
        "\n",
        "\n",
        "\n",
        "# embedding2d_test = mapper2d.transform(X_test)\n",
        "embedding3d_test = mapper3d.transform(X_test)\n",
        "\n",
        "# embedding2d = embedding2d_train.tolist() + embedding2d_test.tolist()\n",
        "embedding3d = embedding3d_train.tolist() + embedding3d_test.tolist()\n",
        "# y_test \n",
        "print(y_test.shape)\n",
        "# y_test = np.where(y_test >= 1, 'RI', 'DE')\n",
        "y_plot = y_train.tolist() + y_test.tolist()\n",
        "bp = Babyplot(background_color=\"#ffffddff\", )\n",
        "bp.add_plot(embedding3d, 'pointCloud', 'categories', y_plot, {\n",
        "    'colorScale': 'Viridis',\n",
        "    'showLegend': True,\n",
        "    'folded': True,\n",
        "    'size': 5,\n",
        "    'showAxes': [True, True, True],\n",
        "    'axisLabels': ['X', 'Y', 'Z'],\n",
        "    'showTickLines': [[True, True], [True, True], [True, True]]\n",
        "})\n",
        "bp.save_as_html(r'/content/test3.html')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU9_sM1Amhg3"
      },
      "source": [
        "import hdbscan\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dataset = raw_dataset.to_numpy()[:,1:-1]\n",
        "labels = raw_dataset.to_numpy()[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tFXKnlQmicB"
      },
      "source": [
        "hdbscan_labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500).fit_predict(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}